## PREDICTOR 1 equation (like the link function)
mu_1 <- intercept + b_hoh79 * x_hoh + b_female * x_female + b_age * x_age + b_nonwhite * x_non_white + b_married * x_married +
b_educ * x_educ + b_ch05 * x_ch05 + b_ch613 * x_ch613 + b_ch1417 * x_ch1417 + b_metro * x_metro
## PREDICTOR 2 equation (like the link function)
mu_2 <- intercept2 + b2_hoh79 * x_hoh + b2_female * x_female + b2_age * x_age + b2_nonwhite * x_non_white +
b2_married * x_married + b2_educ * x_educ + b2_ch05 * x_ch05 + b2_ch613 * x_ch613 + b2_ch1417 * x_ch1417 + b2_metro * x_metro
# simulating the error
rho <- 0.1 # this is the correlation between my linear predictors
drawn_error <- rnorm(n = N, mean = 0, sd = 1)
drawn_error_2 <- rnorm(n = N, mean = 0 + rho * (drawn_error - 0), sd = 1 * sqrt(1 - rho^2))
lin_pred_1 <- mu_1 + drawn_error
lin_pred_2 <- mu_2 + drawn_error_2
## PREDICTOR 2
y <- ifelse(lin_pred_1 < 0, -1, ifelse(lin_pred_2 < 0, 0, 1))
return(y)
})
# confirming that my new matrix is N rows and 1000 columns
dim(my_draws)
# create new summary matrix
draw_summary <- matrix(NA, ncol = 3, nrow = N)
colnames(draw_summary) <- c("val_0", "val_-1", "val_1")
# fill it in based on the values generated
for (i in 1:ncol(my_draws)) {
draw_summary[i, 1] <- length(which(my_draws[i, ] == 0))
draw_summary[i, 2] <- length(which(my_draws[i, ] == -1))
draw_summary[i, 3] <- length(which(my_draws[i, ] == 1))
}
prior_distributions <- draw_summary/rowSums(draw_summary)
plot1 <- ggplot(tibble(neg_one = c(prior_distributions[,c("val_-1")]))) +
geom_density(aes(x = neg_one)) +
ggtitle("Value = -1")
plot2 <- ggplot(tibble(zero = c(prior_distributions[,c("val_0")]))) +
geom_density(aes(x = zero)) +
ggtitle("Value = 0")
plot3 <- ggplot(tibble(val1 = c(prior_distributions[,c("val_1")]))) +
geom_density(aes(x = val1)) +
ggtitle("Value = 1")
(plot1 | plot2) / plot3
x$ilf <- 1 - x$nilf
# make copy of matrix x where we fill in NAs
x_probit <- sapply(x, function(lala) ifelse(is.na(lala) == TRUE, 0, lala)) %>% as.data.frame()
probit_post <- stan_glm(ilf  ~ hoh79 + female + non_white + metro + married + ownchild + ch05 + ch613 + ch1417 + age + educ,
seed = 200,
QR = TRUE,
data = x_probit,
family = binomial(link = "probit"),
init_r = 0.25,
iter = 1000,
chains = 3,
cores = 2)
print(probit_post, digits = 2)
plot(probit_post, pars = c("hoh79", "female", "non_white", "metro", "married", "ownchild", "ch05", "ch613", "ch1417", "age", "educ"))
UBI <- readr::read_csv("http://www2.math.uconn.edu/~valdez/telematics_syn-032021.csv", show_col_types = FALSE)
training <- UBI[1:50000,]
testing <- UBI[-(1:50000),]
training_2 <- training %>%
mutate(sex_dummy = ifelse(Insured.sex == "Male", 1, 0),
marital_dummy = ifelse(Marital == "Married", 1, 0),
region_dummy = ifelse(Region == "Urban", 1, 0),
caruse_urbn_dummy = ifelse(Car.use == "Commute", 1, 0),
caruse_private_dummy = ifelse(Car.use == "Private", 1, 0),
caruse_farmer_dummy = ifelse(Car.use == "Farmer", 1, 0)) %>%
select(-c(Insured.sex, Marital, Car.use, Region))
poisson_post <- stan_glm(NB_Claim ~ Annual.pct.driven + Pct.drive.mon + `Pct.drive.rush am` + Pct.drive.4hrs,
data = training_2,
family = poisson,
seed = 50,
offset = log(Duration),
iter = 1500,
chains = 3)
poisson_post <- stan_glm(NB_Claim ~ Annual.pct.driven + Pct.drive.mon + `Pct.drive.rush am` + Pct.drive.4hrs,
data = training_2,
family = poisson,
seed = 50,
offset = log(Duration),
iter = 1500,
chains = 2)
poisson_post <- stan_glm(NB_Claim ~ Annual.pct.driven + Pct.drive.mon + `Pct.drive.rush am` + Pct.drive.4hrs,
data = training_2,
family = poisson,
seed = 500,
offset = log(Duration),
iter = 1000,
chains = 3)
negbinom_post <- update(poisson_post, family = neg_binomial_2)
plot1 <- pp_check(poisson_post, plotfun = "bars")
plot2 <- pp_check(negbinom_post, plotfun = "bars")
plot1 / plot2
poisson_loo <- loo(poisson_post, k_threshold > 0.7)
poisson_loo <- loo(poisson_post, k_threshold = 0.7)
negbinom_loo <- loo(negbinom_post, k_threshold = 0.7)
plot(poisson_loo, label_points = TRUE)
plot(negbinom_loo, label_points = TRUE)
negbinom_loo <- loo(negbinom_post)
negbinom_loo <- loo(negbinom_post)
plot(negbinom_loo, label_points = TRUE)
plot(negbinom_loo, label_points = TRUE)
plot(negbinom_loo, label_points = TRUE)
pois_ll <- log_lik(poisson_post, newdata = testing)
negbinom_ll <- log_lik(negbinom_post, newdata = testing)
loo_compare(pois_ll, negbinom_ll)
plot(negbinom_loo, label_points = TRUE)
pois_ll <- rstanarm::log_lik(poisson_post, offset = log(Duration), newdata = testing)
pois_ll <- rstanarm::log_lik(poisson_post, offset = log(testing$Duration), newdata = testing)
negbinom_ll <- rstanarm::log_lik(negbinom_post, offset = log(testing$Duration), newdata = testing)
loo_compare(pois_ll, negbinom_ll)
pois_ll <- rstanarm::log_lik(poisson_post, offset = c(0, 0), newdata = testing)
pois_ll <- rstanarm::log_lik(poisson_post, offset = matrix(0,nrow = 50000), newdata = testing)
loo_compare(pois_ll, negbinom_ll)
negbinom_ll <- rstanarm::log_lik(negbinom_post, offset = matrix(0,nrow = 50000), newdata = testing)
loo_compare(pois_ll, negbinom_ll)
rowSum(pois_ll)
rowSums(pois_ll)
gamma_post <- stan_glm(NB_Claim ~ Car.age + Years.noclaims + Annual.pct.driven + Pct.drive.mon  + `Pct.drive.rush am` +
Pct.drive.wkend + Accel.12miles + Left.turn.intensity12 + Right.turn.intensity12 + sex_dummy,
data = training_2,
offset = log(Duration),
family = Gamma(link = "log"),
prior_PD = TRUE)
str(training_2)
summary(training_2)
gamma_post <- stan_glm(NB_Claim ~ Annual.pct.driven + Pct.drive.mon  + `Pct.drive.rush am` +
Pct.drive.wkend + Accel.12miles + Left.turn.intensity12 + Right.turn.intensity12 + sex_dummy,
data = training_2,
offset = log(Duration),
family = Gamma(link = "log"),
prior_PD = TRUE)
gamma_post <- stan_glm(NB_Claim ~ Annual.pct.driven + Pct.drive.mon  + `Pct.drive.rush am` +
Pct.drive.wkend + Accel.12miles + Left.turn.intensity12 + Right.turn.intensity12,
data = training_2,
offset = log(Duration),
family = Gamma(link = "log"),
prior_PD = TRUE)
summary(test)
training_subset <- training_2 %>%
filter(NB_claim > 0) %>%
mutate(AMT_per_claim = AMT_claim / NB_claim)
training_subset <- training_2 %>%
filter(NB_Claim > 0) %>%
mutate(AMT_per_claim = AMT_claim / NB_Claim)
training_subset <- training_2 %>%
filter(NB_Claim > 0) %>%
mutate(AMT_per_claim = AMT_Claim / NB_Claim)
gamma_post <- stan_glm(AMT_per_Claim ~ Annual.pct.driven + Pct.drive.mon  + `Pct.drive.rush am` +
Pct.drive.wkend + Accel.12miles + Left.turn.intensity12 + Right.turn.intensity12 + sex_dummy,
data = training_2,
offset = log(Duration),
family = Gamma(link = "log"),
prior_PD = FALSE)
gamma_post <- stan_glm(AMT_per_Claim ~ Annual.pct.driven + Pct.drive.mon  + `Pct.drive.rush am` +
Pct.drive.wkend + Accel.12miles + Left.turn.intensity12 + Right.turn.intensity12 + sex_dummy,
data = training_subset,
offset = log(Duration),
family = Gamma(link = "log"),
prior_PD = FALSE)
gamma_post <- stan_glm(AMT_per_claim ~ Annual.pct.driven + Pct.drive.mon  + `Pct.drive.rush am` +
Pct.drive.wkend + Accel.12miles + Left.turn.intensity12 + Right.turn.intensity12 + sex_dummy,
data = training_subset,
offset = log(Duration),
family = Gamma(link = "log"),
prior_PD = FALSE)
summary(training_subset)
# poisson
print(mean(rowSums(pois_ll)))
# negative binomial
print(mean(rowSums(negbinom_ll)))
loo_compare(poisson_loo, negbinom_loo)
gamma_post <- stan_glm(NB_Claim ~ Annual.pct.driven + Pct.drive.mon  + `Pct.drive.rush am` +
Pct.drive.wkend + Accel.12miles + Left.turn.intensity12 + Right.turn.intensity12 + sex_dummy,
data = training_2,
offset = log(Duration),
family = Gamma(link = "log"),
prior_PD = TRUE)
at_least_1 <- training_2 %>% filter(NB_Claim > 0)
gamma_post <- stan_glm(NB_Claim ~ Annual.pct.driven + Pct.drive.mon  + `Pct.drive.rush am` +
Pct.drive.wkend + Accel.12miles + Left.turn.intensity12 + Right.turn.intensity12 + sex_dummy,
data = at_least_1,
offset = log(Duration),
family = Gamma(link = "log"),
prior_PD = TRUE)
gamma_prior <- stan_glm(NB_Claim ~ Annual.pct.driven + Pct.drive.mon  + `Pct.drive.rush am` +
Pct.drive.wkend + Accel.12miles + Left.turn.intensity12 + Right.turn.intensity12 + sex_dummy,
data = at_least_1,
offset = log(Duration),
family = Gamma(link = "log"),
prior_PD = TRUE)
print(gamma_prior)
gamma_prior <- stan_glm(AMT_Claim ~ Annual.pct.driven + Pct.drive.mon  + `Pct.drive.rush am` +
Pct.drive.wkend + Accel.12miles + Left.turn.intensity12 + Right.turn.intensity12 + sex_dummy,
data = at_least_1,
offset = log(Duration),
family = Gamma(link = "log"),
prior_PD = TRUE)
summary(at_least_1$AMT_Claim)
at_least_1 <- training_2 %>% filter(NB_Claim > 0) %>% mutate(AMT_Claim = ifelse(AMT_Claim == 0, 0.01, AMT_Claim)
gamma_prior <- stan_glm(AMT_Claim ~ Annual.pct.driven + Pct.drive.mon  + `Pct.drive.rush am` +
at_least_1 <- training_2 %>% filter(NB_Claim > 0) %>% mutate(AMT_Claim = ifelse(AMT_Claim == 0, 0.01, AMT_Claim))
gamma_prior <- stan_glm(AMT_Claim ~ Annual.pct.driven + Pct.drive.mon  + `Pct.drive.rush am` +
Pct.drive.wkend + Accel.12miles + Left.turn.intensity12 + Right.turn.intensity12 + sex_dummy,
data = at_least_1,
offset = log(Duration),
family = Gamma(link = "log"),
prior_PD = TRUE)
print(gamma_prior)
pp_check(gamma_prior)
pp_check(gamma_prior, fun = "bars")
pp_check(gamma_prior, plotfun = "bars")
pp_check(gamma_prior, plotfun = "scatter_avg")
pp_check(gamma_prior)
pp_check(gamma_prior, plotfun = "scatter_avg")
View(at_least_1)
at_least_1 <- training_2 %>% filter(NB_Claim > 0, AMT_Claim > 0)
gamma_prior <- stan_glm(AMT_Claim ~ Annual.pct.driven + Pct.drive.mon  + `Pct.drive.rush am` +
Pct.drive.wkend + Accel.12miles + Left.turn.intensity12 + Right.turn.intensity12 + sex_dummy,
data = at_least_1,
offset = log(Duration),
family = Gamma(link = "log"),
prior_PD = TRUE)
print(gamma_prior)
pp_check(gamma_prior, plotfun = "scatter_avg")
training_subset <- training_2 %>%
filter(NB_Claim > 0, ) %>%
mutate(AMT_per_claim = AMT_Claim / NB_Claim)
gamma_post <- stan_glm(AMT_per_claim ~ Annual.pct.driven + Pct.drive.mon  + `Pct.drive.rush am` +
Pct.drive.wkend + Accel.12miles + Left.turn.intensity12 + Right.turn.intensity12 + sex_dummy,
data = training_subset,
offset = log(Duration),
family = Gamma(link = "log"),
prior_PD = FALSE)
training_subset <- training_2 %>%
filter(NB_Claim > 0, AMT_CLaim > 0) %>%
mutate(AMT_per_claim = AMT_Claim / NB_Claim)
training_subset <- training_2 %>%
filter(NB_Claim > 0, AMT_Claim > 0) %>%
mutate(AMT_per_claim = AMT_Claim / NB_Claim)
gamma_post <- stan_glm(AMT_per_claim ~ Annual.pct.driven + Pct.drive.mon  + `Pct.drive.rush am` +
Pct.drive.wkend + Accel.12miles + Left.turn.intensity12 + Right.turn.intensity12 + sex_dummy,
data = training_subset,
offset = log(Duration),
family = Gamma(link = "log"),
prior_PD = FALSE)
pp_check(gamma_post, plotfun = "scatter_avg")
gamma_prior_predictions <- posterior_predict(gamma_prior)
ggplot(tibble(AMT = c(gamma_prior_predictions))) + geom_density(aes(x = AMT))
Sys.getenv("BINPREF")
install.packages("RTools")
writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
Sys.which("make")
Sys.which("make")
library(RTools)
Sys.which("make")
Sys.getenv("BINPREF")
writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
Sys.which("make")
install.packages("RTools")
?version
R.Version
R.version$os
R.version
write('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', file = "~/.Renviron", append = TRUE)
Sys.which("make")
library(haven)
library(brms)
library(rstanarm)
library(dplyr)
set.seed(1234)
options(mc.cores = parallel::detectCores())
library(haven)
library(brms)
library(rstanarm)
library(dplyr)
set.seed(1234)
options(mc.cores = parallel::detectCores())
if (!file.exists("GSS_2020_panel_stata_1a.zip")) {
download.file("https://gss.norc.org/Documents/stata/GSS_2020_panel_stata_1a.zip",
destfile = "GSS_2020_panel_stata_1a.zip")
unzip("GSS_2020_panel_stata_1a.zip")
}
GSS <- as_factor(read_dta(file.path("gss2020panel_r1a.dta")))
#unzip("anes_timeseries_2020_gss_stata_20220408.zip")
ANES <- as_factor(read_dta(file.path("anes_timeseries_2020_gss_stata_20220408.dta")))
ANES_GSS <- inner_join(ANES, GSS, by = c(YEARID = "yearid"))
# clean up data set to remove
ANES_GSS_clean <- ANES_GSS %>%
filter(!is.na(degree_1b)) %>%
filter(V202215 != "-9. Refused" &
V202215 != "-5. Interview breakoff (sufficient partial IW)") %>%
mutate(V202215_ordered = as.ordered(V202215),
degree_1b_ordered = as.ordered(factor(degree_1b,
levels = c("graduate",
"bachelor's",
"associate/junior college",
"high school",
"less than high school")))) %>%
select(V202215_ordered, degree_1b_ordered)
prior1 <- brms::brm(V202215_ordered ~ mo(degree_1b_ordered),
data = ANES_GSS_clean,
sample_prior = "only",
family = cumulative(link = "probit"),
prior = prior(normal(0,100), class = "b"),
seed = 10001,
iter = 5000,
chains = 4)
prior1
prior_draws <- posterior_predict(prior1)
summary(colMeans(prior_draws))
post <- brms::brm(V202215_ordered ~ mo(degree_1b_ordered),
data = ANES_GSS_clean,
family = cumulative(link = "probit"),
prior = prior(normal(0,100), class = "b"),
seed = 10001,
chains = 2)
post
plot(conditional_effects(post, effects = "degree_1b_ordered", categorical = FALSE))
plot(loo(post))
alt_post <- brms::brm(V202215_ordered ~ degree_1b_ordered,
data = ANES_GSS_clean,
family = cumulative(link = "probit"),
prior = prior(normal(0,100), class = "b"),
seed = 10001,
chains = 3)
plot(loo(alt_post))
post <- add_criterion(post, criterion = "loo")
alt_post <- add_criterion(alt_post, criterion = "loo")
loo_compare(post, alt_post)
posterior_draws <- posterior_predict(post)
summary(colMeans(posterior_draws))
summary(ANES_GSS_clean$degree_1b_ordered)
summary(ANES_GSS_clean$V202215_ordered)
shiny::runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
library(plotly)
shiny::runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
shiny::runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
runApp()
runApp('GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft')
### load data sources ###
single_family_homes <- read.csv("../data/single_family_homes_time_series.csv")
HPI <- read.csv("../data/HPI_data.csv")
bottom_tier <- read.csv("../data/all_homes_bottom_tier.csv")
setwd("~/GitHub/DataViz/Group-E_Climate-Real-Estate/Shiny Draft")
### load data sources ###
single_family_homes <- read.csv("../data/single_family_homes_time_series.csv")
HPI <- read.csv("../data/HPI_data.csv")
bottom_tier <- read.csv("../data/all_homes_bottom_tier.csv")
ZCTA_list <- c('70112', '70113', '70114', '70115', '70116', '70117', '70118', '70119',
'70121', '70122', '70123', '70124', '70125', '70126', '70127', '70128',
'70129', '70130','70131', '70139', '70163', '95401', '95403', '95404',
'95405', '95407', '95409', '14201', '14202', '14203', '14204', '14206',
'14207', '14208', '14209', '14210', '14211', '14212', '14213', '14214',
'14215', '14216', '14217', '14218', '14219', '14220', '14221','14222',
'14223', '14224', '14225', '14226', '14227', '14228', '14261', '73160',
'73165', '73170', '70358')
new_orleans_zips <- c('70112', '70113', '70114', '70115', '70116', '70117', '70118', '70119',
'70121', '70122', '70123', '70124', '70125', '70126', '70127', '70128',
'70129', '70130','70131', '70139', '70163')
moore_zips <- c("73160", "73165", "73170")
coffey_zips <- c("95401", "95402", "95403", "95404", "95405", "95406", "95407", "95409")
buffalo_zips <- c('14201', '14202', '14203','14204', '14206', '14207', '14208',
'14209', '14210', '14211', '14212', '14213', '14214', '14215', '14216',
'14217', '14218', '14219', '14220', '14221','14222', '14223', '14224',
'14225', '14226', '14227', '14228', '14261')
grand_isle_zips <- c("70358")
## HPI ##
HPI <- HPI %>%
filter(zip_code %in% ZCTA_list) # refine the list so the data is smaller
months <- expand.grid(year = unique(HPI$year),
month = c("01","02","03","04","05","06",
"07", "08", "09", "10", "11", "12"))
HPI <- left_join(HPI, months, by = "year") %>%
mutate(date = as.Date(paste0(month, "/01/", year), "%m/%d/%Y")) %>%
select(zip_code, date, HPI_2000, HPI, annual_change) %>%
mutate(zip_code = as.character(zip_code),
HPI_2000 = as.numeric(HPI_2000),
HPI = as.numeric(HPI),
annual_change = as.numeric(annual_change))
# a curated list of zip codes missing from our data
missing_zips <- c('70112', '14203', '14204', '14208') %>%
as.data.frame() %>%
rename(zip_code = '.') %>%
mutate(date = NA,
single_fam_val = NA,
bottom_tier = NA,
HPI_2000 = NA,
HPI = NA,
annual_change = NA)
# join all the real estate data #
base_data <- single_family_homes %>%
left_join(bottom_tier, by = c("region_id", "date")) %>%
select(region_id, date, value.x, value.y) %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
mutate(date = date + 1) %>% # add a day so that it begins on the first of the month
rename("zip_code" = "region_id",
"single_fam_val" = "value.x",
"bottom_tier" = "value.y") %>%
mutate(zip_code = as.character(zip_code)) %>%
filter(zip_code %in% ZCTA_list) %>% # refine the list so the data is smaller
full_join(HPI, by = c("zip_code", "date")) %>%
filter(date > as.Date('2000-01-01', "%Y-%m-%d") & date < as.Date('2022-01-01', "%Y-%m-%d")) %>%
rbind(missing_zips) %>%
tidyr::complete(date, zip_code) %>%
arrange(zip_code, date) %>%
filter(is.na(date) == FALSE) %>%
mutate(city = case_when(zip_code %in% c('70112', '70113', '70114', '70115', '70116', '70117',
'70118', '70119', '70121', '70122', '70123', '70124',
'70125', '70126', '70127', '70128', '70129', '70130',
'70131', '70139', '70163') ~ "New Orleans",
zip_code %in% c("73160", "73165", "73170") ~ "Moore",
zip_code %in% c("95401", "95402", "95403", "95404", "95405", "95406",
"95407", "95409") ~ "Coffey Park",
zip_code %in% c('14201', '14202', '14203','14204', '14206', '14207', '14208',
'14209', '14210', '14211', '14212', '14213', '14214', '14215',
'14216', '14217', '14218', '14219', '14220', '14221','14222',
'14223', '14224', '14225', '14226', '14227', '14228', '14261') ~ "Buffalo",
zip_code == '70358' ~ "Grand Isle"))# create months for zip codes missing from our data set
impute_data_zip <- base_data %>%
group_by(city, date) %>%
summarize(avg_single_fam_val = mean(single_fam_val, na.rm = TRUE),
avg_bottom_tier = mean(bottom_tier, na.rm = TRUE),
avg_HPI_2000 = mean(HPI_2000, na.rm = TRUE),
avg_HPI = mean(HPI, na.rm = TRUE),
avg_annual_change = mean(annual_change, na.rm = TRUE)) %>%
ungroup() %>%
group_by(city) %>%
mutate(avg_annual_change = case_when(is.na(avg_annual_change) == TRUE ~ as.numeric(lag(avg_single_fam_val,0)/lag(avg_single_fam_val,1)-1)*100,
TRUE ~ as.numeric(avg_annual_change))) %>%
ungroup()
# impute the missing zip codes' numbers using the average for the city
base_data <- base_data %>%
left_join(impute_data_zip, by = c("city", "date")) %>%
mutate(single_fam_val = case_when(is.na(single_fam_val) == TRUE ~ avg_single_fam_val,
TRUE ~ as.numeric(single_fam_val)),
bottom_tier = case_when(is.na(bottom_tier) == TRUE ~ avg_bottom_tier,
TRUE ~ as.numeric(bottom_tier)),
HPI_2000 = case_when(is.na(HPI_2000) == TRUE ~ avg_HPI_2000,
TRUE ~ as.numeric(HPI_2000)),
HPI = case_when(is.na(HPI) == TRUE ~ avg_HPI,
TRUE ~ as.numeric(HPI)),
annual_change = case_when(is.na(annual_change) == TRUE ~ avg_annual_change,
TRUE ~ as.numeric(annual_change))) %>%
select(date, zip_code, single_fam_val, bottom_tier, HPI_2000, HPI, annual_change, city)
line_chart_data <- impute_data_zip
## bar chart data ##
new_orleans_diff <- base_data %>%
filter(city == "New Orleans",
date %in% c(as.Date("06/01/2005",  "%m/%d/%Y"),
as.Date("08/01/2005",  "%m/%d/%Y"),
as.Date("10/01/2005",  "%m/%d/%Y")))
moore_diff <- base_data  %>%
filter(city == "Moore",
date %in% c(as.Date("08/01/2017",  "%m/%d/%Y"),
as.Date("10/01/2017",  "%m/%d/%Y"),
as.Date("12/01/2017",  "%m/%d/%Y")))
buffalo_diff <- base_data  %>%
filter(city == "Buffalo",
date %in% c(as.Date("09/01/2014",  "%m/%d/%Y"),
as.Date("11/01/2014",  "%m/%d/%Y"),
as.Date("01/01/2015",  "%m/%d/%Y")))
grandisle_diff <- base_data  %>%
filter(city == "Grand Isle",
date %in% c(as.Date("02/01/2010",  "%m/%d/%Y"),
as.Date("04/01/2010",  "%m/%d/%Y"),
as.Date("06/01/2010",  "%m/%d/%Y")))
coffey_diff <- base_data  %>%
filter(city == "Coffey Park",
date %in% c(as.Date("08/01/2017",  "%m/%d/%Y"),
as.Date("10/01/2017",  "%m/%d/%Y"),
as.Date("12/01/2017",  "%m/%d/%Y")))
bar_chart_data <- new_orleans_diff %>%
bind_rows(coffey_diff, moore_diff, grandisle_diff, buffalo_diff)
rm(new_orleans_diff, coffey_diff, moore_diff, grandisle_diff, buffalo_diff)
# NEW ORLEANS #
new_orleans_shape <- readOGR("../data/shape files", "New_Orleans")
new_orleans_shape <- new_orleans_shape[,-c(2,3,4,5,6,7,8,9)]
refined_orleans_data <- new_orleans_shape %>%
rename(zip_code = ZCTA5CE10) %>%
spTransform(CRS("+proj=longlat +datum=WGS84 +no_defs")) %>%
merge(filter(base_data, zip_code %in% new_orleans_zips),
by = "zip_code",
duplicateGeoms = T) %>% st_as_sf()
neworleans_line_data <- line_chart_data %>%
filter(city == 'New Orleans')
avg_metric_neworleans = paste0('avg_', chosen_metric_neworleans)
neworleans_line_data$date2 <- format(neworleans_line_data$date, format="%Y %m")
neworleans_line_data2 <- neworleans_line_data %>%
filter(between(date,
as.Date("2004-08-01", format = "%Y-%m-%d"),
as.Date("2006-08-01", format = "%Y-%m-%d"))) %>%
select(date, date2, avg_metric_neworleans) %>%
rename(selected_metric = avg_metric_neworleans)
View(neworleans_line_data)
filter(neworleans_line_data, zip_code == '70112')
View(neworleans_line_data)
View(bar_chart_data)
View(bar_chart_data)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
